---
title: "Open Research Week 2021: Tools And Technology For Open Research - Research Analytical"
author: Jacob L. Macdonald, University of Liverpool; Jacob.Macdonald@liverpool.ac.uk
output: rmarkdown::github_document
always_allow_html: true
knit: (function(inputFile, encoding) { rmarkdown::render(inputFile, encoding = encoding, output_file = paste0(dirname(inputFile),'/README.md')) })
---

```{r Project Setup, include=FALSE}
library(data.table)
library(sf)
library(randomcoloR)
library(ggpubr)
library(gridExtra)
library(ggplot2)
library(dplyr)
library(tidygraph)
library(numDeriv)
library(stringr)
library(grid)

knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)

wd <- paste0(ifelse(Sys.info()[[1]]=="Linux", paste0("/home/", Sys.info()[[7]], "/Documents"), paste0("/Users/", Sys.info()[[7]])), "/Dropbox/Research/OpenResearchWeek_2021")
wd.data <- paste0(wd, "/Data")
```


## Getting The Results Out There
*Part 2 of Open Research Week 2021 Talk - Tools and Technology for Open Research*

Using some work on open data, this research analytical showcases an online presentation of outputs, visualization, code and analysis. We'll see a side by side comparison of the code and the output on github to give an example of how different research deliverables can be shared.

**Github** + **RMarkdown** are great tools to use together for making research and outputs accessible. This is one way in which we can combine the best of the statistical and analytical capabilities of *R* with the tracking, presentation and collaborative nature of Github.

Primarily, **Git** and **Github** are used to monitor and version control coding documents. This has developed itself into a hosting site for a wide range and types of outputs. 

During this session, we'll overview the main ways and examples of how to incorporate:
- Text 
- Code
- Visualization
- Methodologies
- Analyses

One of the main benefits of this format is the sharing of code and data outputs

```{r Rmd Code Example}
set.seed(2021)
x <- data.frame(Type="Normal", Value=rnorm(1000, 2, 1))
set.seed(2021)
y <- data.frame(Type="Chi-Sq.", Value=rchisq(1000, 5))

distributions <- rbind(x, y)

p <- ggplot(distributions, aes(x=Value, fill=Type)) +
  geom_density(alpha=0.4) +
  scale_fill_manual(values=c("#999999", "#E69F00", "#56B4E9")) +
  theme(legend.position="bottom", legend.title = element_blank())
p
```



## Section 1: Parametric Employment Subcentre Identification for Great Britain

**A geographic data science open research application**


Urban metropolitan areas are often seen to comprise clusters of employment over space; often with a primary nucleus and surrounding satellite *subcentres* where employment densities are strong.


Stylized facts across the globe have given rise to a large literature on the *Monocentric City Model* where a city's employment density decreases at some functional rate from a singular central business district.


Increasingly granular (census tract) and accessible data allow us to explore the validity of these theoretic models, and more generally apply their concepts to real world data to explore employment (or population) densities. Identifying patterns in the cluster and spatial configuration of the data.


**Can we build a programatic way to efficiently explore a wide variety of flexible models across a range of urban areas.**
**Based only on openly accessible tools and software.**


Update to code for exisiting employment subcentre identification in LA and tailored to the local England context [Ban et al. 2017](https://www.mdpi.com/2073-445X/6/1/17/htm "Identifying Employment Subcenters: The Method of Exponentially Declining Cutoffs").  



### 1.1. Monocentric Employment Density Decay


$$D_x = a \cdot e^{f(x)}$$

* $D_x$: employment density of a given area (census tract) employment levels per hectare
* $x$: distance of the area to the central business district (CBD); i.e. local region's densest census tract
* $f(x)$: some function of distance to the CBD (located at $x=0$) where $f'(x)<0$

Here $a$ represents some baseline value of employment or population density at the central location,

An employment density gradient can be estimated on these data to define the shape of how local employment decays (as a function of distance to the CBD).

The functional form of $f(x)$ defines the decaying pattern of how we would expect densities to decline moving away from the dense CBD.

In log form $\ln(D_x)$, this function represents the proportional decay of density as influenced by distance to the CBD.

$$\ln(D_x)=\ln(a)+f(x)$$

In [Ban et al. 2017](https://www.mdpi.com/2073-445X/6/1/17/htm "Identifying Employment Subcenters: The Method of Exponentially Declining Cutoffs"), an algorithm is developed to find clusters of census tracts which have employment densities higher than what this monocentric decay would predict - i.e. peripheral subcentre employment zones.



### 1.2. Functional Form of Employment Density Gradients


The way in which we assume employment density patterns will decay from this central point is important, and can have important consequences.
*i.e. the functional form of $f(x)$*


Efficient, open data and code allows us to build tools to quickly test and validate different models, assumptions, parameters and outputs.


Updating the existing open code to allow for more flexible parameter choice and more flexibility in choosing the assumptions behind what we expect employment density decay to look like. 


The ability to be able to do this completely start to finish for many regions across a country, automatized, comparably, fast, and for free - shows the usefulness of how open research can be used for deep analysis and increasingly robust work.


We explore four common density gradients across different regions in England.


(a, b) $$f(x) = \alpha - \beta_1 \cdot x$$
 
```{r Gradient Decay: Linear}
x <- 1:10

p1 <- ggplot(data.frame(x, y = 6*exp(-x)), aes(x,y)) +
  stat_function(fun=function(x) 6*exp(-x), colour="gray65") +
  scale_y_continuous(limits = c(0, 6), expand = c(0, 0)) +
  scale_x_continuous(limits = c(0, 6), expand = c(0, 0)) +
  labs(y = "Density", x = "Distance to CBD (x)", caption="",
    title = "Employment Density Gradients", subtitle = expression(paste("(a) ", D==a,e^-b[1],""^x))) +
  theme_classic() +
  theme(axis.text.x=element_blank(), axis.ticks.x=element_blank(), 
    axis.text.y=element_blank(), axis.ticks.y=element_blank())
p2 <- ggplot(data.frame(x, y = 5.75-x), aes(x,y)) +
  stat_function(fun=function(x) 5.75-x, colour="gray65") +
  scale_y_continuous(limits = c(0, 6), expand = c(0, 0)) +
  scale_x_continuous(limits = c(0, 6), expand = c(0, 0)) +
  labs(y = "log Density", x = "Distance to CBD (x)", caption="Linear f(x) proportional decay",
    title = "", subtitle = expression(paste("(b) ", ln(D)==ln(a)-b[1],x))) +
  theme_classic() +
  theme(axis.text.x=element_blank(), axis.ticks.x=element_blank(), 
    axis.text.y=element_blank(), axis.ticks.y=element_blank(), plot.caption = element_text(face = "italic"))
grid.arrange(p1, p2, ncol=2)
```


(c, d) $$f(x) = \alpha + \beta_1 \cdot x - \beta_2 \cdot x^2$$

```{r Gradient Decay: Squared1, include=FALSE}
p1 <- ggplot(data.frame(x, y = 2*exp(0.2+0.5*x-x^2)), aes(x,y)) +
  stat_function(fun=function(x) 2*exp(0.2+0.5*x-x^2), colour="gray65") +
  scale_y_continuous(limits = c(0, 2.8), expand = c(0, 0)) +
  scale_x_continuous(limits = c(0, 3), expand = c(0, 0)) +
  labs(y = "Density", x = "Distance to CBD (x)", caption="", subtitle = expression(paste("(c) ", D==a,e^b[1],""^x,""^-b[2],""^x^2))) +
  theme_classic() +
  theme(axis.text.x=element_blank(), axis.ticks.x=element_blank(),
    axis.text.y=element_blank(), axis.ticks.y=element_blank())
p2 <- ggplot(data.frame(x, y = log(5*exp(x-x^2))), aes(x,y)) +
  stat_function(fun=function(x) log(5*exp(x-x^2)), colour="gray65") +
  scale_y_continuous(limits = c(0, 2), expand = c(0, 0)) +
  scale_x_continuous(limits = c(0, 2), expand = c(0, 0)) +
  labs(y = "log Density", x = "Distance to CBD (x)", caption="Quadratic f(x) proportional decay",
    subtitle = expression(paste("(d) ", ln(D)==ln(a)+b[1],x,-b[2],x^2))) +
  theme_classic() +
  theme(axis.text.x=element_blank(), axis.ticks.x=element_blank(),
    axis.text.y=element_blank(), axis.ticks.y=element_blank(), plot.caption = element_text(face = "italic"))
```

```{r Gradient Decay: Squared1 Print}
grid.arrange(p1, p2, ncol=2)
```


(e, f) $$f(x) = \alpha - \beta_1 \cdot x + \beta_2 \cdot x^2$$

```{r Gradient Decay: Squared2, include=FALSE}
#   #stat_function(fun=function(x) 10-8*x+(x*x), colour="black", xlim = c(1, 4)) +
#   #stat_function(fun=function(x) 10-8*x+(x*x), colour="black", xlim = c(4, 5), linetype = "dashed") +

p1 <- ggplot(data.frame(x, y = 3*exp(-3.4*x+0.9*x^2)), aes(x,y)) +
  stat_function(fun=function(x) 3*exp(-3.4*x+0.9*x^2), colour="gray65", xlim = c(0, 2)) +
  stat_function(fun=function(x) 3*exp(-3.4*x+0.9*x^2), colour="gray65", xlim = c(2, 3), linetype = "dashed") +
  scale_y_continuous(limits = c(0, 3.2), expand = c(0, 0)) +
  scale_x_continuous(limits = c(0, 3), expand = c(0, 0)) +
  labs(y = "Density", x = "Distance to CBD (x)", caption="", subtitle = expression(paste("(e) ", D==a,e^-b[1],""^x,""^+b[2],""^x^2))) +
  theme_classic() +
  theme(axis.text.x=element_blank(), axis.ticks.x=element_blank(),
    axis.text.y=element_blank(), axis.ticks.y=element_blank())
p2 <- ggplot(data.frame(x, y = log(20*exp(-4.8*x+1.95*x^2) ) ), aes(x,y)) +
  stat_function(fun=function(x) log(20*exp(-4.8*x+1.95*x^2) ), colour="gray65", xlim = c(0, 1.23)) +
  stat_function(fun=function(x) log(20*exp(-4.8*x+1.95*x^2) ), colour="gray65", xlim = c(1.23, 2), linetype = "dashed") +
  scale_y_continuous(limits = c(0, 3), expand = c(0, 0)) +
  scale_x_continuous(limits = c(0, 2), expand = c(0, 0)) +
  labs(y = "log Density", x = "Distance to CBD (x)", caption="Quadratic f(x) proportional decay",
    subtitle = expression(paste("(f) ", ln(D)==ln(a)-b[1],x,+b[2],x^2))) +
  theme_classic() +
  theme(axis.text.x=element_blank(), axis.ticks.x=element_blank(),
    axis.text.y=element_blank(), axis.ticks.y=element_blank(), plot.caption = element_text(face = "italic"))
```

```{r Gradient Decay: Squared2 Print}
grid.arrange(p1, p2, ncol=2)
```


(g, h) $$f(x) = \alpha - \beta_1 \cdot x + \beta_2 \cdot x^2 - \beta_3 \cdot x^3$$

```{r Gradient Decay: Cubic, include=FALSE}
p1 <- ggplot(data.frame(x, y = 0.75*exp(-1*x+3.7*x^2-9*x^3) ), aes(x,y)) +
  stat_function(fun=function(x) 0.75*exp(-1*x+3.7*x^2-9*x^3), colour="gray65") +
  scale_y_continuous(limits = c(0, 0.8), expand = c(0, 0)) +
  scale_x_continuous(limits = c(0, 1), expand = c(0, 0)) +
  labs(y = "Density", x = "Distance to CBD (x)", caption="", subtitle = expression(paste("(g) ", D==a,e^c-b[1],""^x,""^+b[2],""^x^2,""^-b[3],""^x^3))) +
  theme_classic() +
  theme(axis.text.x=element_blank(), axis.ticks.x=element_blank(),  axis.title.x=element_blank(),
    axis.text.y=element_blank(), axis.ticks.y=element_blank())
p2 <- ggplot(data.frame(x, y = log(5*exp(-2*x+4*x^2-4*x^3) ) ), aes(x,y)) +
  stat_function(fun=function(x) log(5*exp(-2*x+4*x^2-4*x^3) ), colour="gray65") +
  scale_y_continuous(limits = c(0, 2.7), expand = c(0, 0)) +
  scale_x_continuous(limits = c(0, 1), expand = c(0, 0)) +
  labs(y = "log Density", x = "Distance to CBD (x)", caption="Cubic f(x) proportional decay", subtitle = expression(paste("(h) ", ln(D)==ln(a)-b[1],x,+b[2],x^2,-b[3],x^3))) +
  theme_classic() +
  theme(axis.text.x=element_blank(), axis.ticks.x=element_blank(),  axis.title.x=element_blank(),
    axis.text.y=element_blank(), axis.ticks.y=element_blank(), plot.caption = element_text(face = "italic"))
grid.arrange(p1, p2, ncol=2)
```

```{r Gradient Decay: Cubic Print}
grid.arrange(p1, p2, ncol=2)
```


(i, j) $$f(x) = \alpha - \beta_4 \cdot \ln(x)$$

```{r Gradient Decay: Log, include=FALSE}
p1 <- ggplot(data.frame(x, y = exp(2-log(0.8*x))), aes(x,y)) +
  stat_function(fun=function(x) 2-log(1.2*x), colour="gray65") +
  scale_y_continuous(limits = c(0, 5), expand = c(0, 0)) +
  scale_x_continuous(limits = c(0, 5), expand = c(0, 0)) +
  labs(y = "Density", x = "Distance to CBD (x)", caption="",
    subtitle = expression(paste("(i) ", D==a,e^-b[1],""^x,""^+b[2],""^x^2,""^-b[3],""^x^3))) +
  theme_classic() +
  theme(axis.text.x=element_blank(), axis.ticks.x=element_blank(),
    axis.text.y=element_blank(), axis.ticks.y=element_blank())
p2 <- ggplot(data.frame(x, y = log(5*exp(-2*x+4*x^2-4*x^3) ) ), aes(x,y)) +
  stat_function(fun=function(x) log(5*exp(-2*x+4*x^2-4*x^3) ), colour="gray65") +
  scale_y_continuous(limits = c(0, 2.7), expand = c(0, 0)) +
  scale_x_continuous(limits = c(0, 1), expand = c(0, 0)) +
  labs(y = "log Density", x = "Distance to CBD (x)", caption="Logarithmic f(x) proportional decay",
    subtitle = expression(paste("(j) ", ln(D)==ln(a)-b[1],x,+b[2],x^2,-b[3],x^3))) +
  theme_classic() +
  theme(axis.text.x=element_blank(), axis.ticks.x=element_blank(),
    axis.text.y=element_blank(), axis.ticks.y=element_blank(), plot.caption = element_text(face = "italic"))
grid.arrange(p1, p2, ncol=2)
```

```{r Gradient Decay: Log Print}
grid.arrange(p1, p2, ncol=2)
```



We're going to thus update the subcentre employment zone algorithm from [Ban et al. 2017](https://www.mdpi.com/2073-445X/6/1/17/htm "Identifying Employment Subcenters: The Method of Exponentially Declining Cutoffs") and then using the flexible update, apply this quickly and efficiently across multiple municipal regions across England. 



## Section 2: Data and Study Region

```{r WZ Employment Data Import, include=FALSE}
London <- fread(paste0(wd.data, "/Local_Authority_District_to_Region__December_2018__Lookup_in_England.csv"))
London <- London[London$RGN18NM=="London",]
London <- unique(London$LAD18CD)

regions <- fread(paste0(wd.data, "/Local_Authority_District_to_Combined_Authority_(December_2019)_Lookup_in_England.csv"))
regions <- split(regions, regions$CAUTH19NM)
regions <- lapply(regions, function(x) unique(x$LAD19CD))
regions[["London"]] <- London
rm(London)

regions <- as.data.table(do.call(rbind, lapply(as.list(1:length(regions)), function(x) cbind(names(regions)[x], regions[[x]]))))
names(regions) <- c("Region", "LAD_CD")

save.names <- as.data.frame(rbind(cbind("Cambridgeshire and Peterborough", "CmP"),
  cbind("Greater Manchester", "MAN"),
  cbind("Liverpool City Region", "LIV"),
  cbind("London", "LND"),
  cbind("North East", "NrE"),
  cbind("North of Tyne", "NrT"),
  cbind("Sheffield City Region", "SHF"),
  cbind("Tees Valley", "TsV"),
  cbind("West Midlands", "WsM"),
  cbind("West of England", "WsE"),
  cbind("West Yorkshire", "WsY")))
names(save.names) <- c("Region", "Save_Name") 

regions <- merge(regions, save.names, all.x=TRUE, sort=FALSE, by="Region")
rm(save.names)

WZ.db <- st_transform(st_read(paste0(wd.data, "/Workplace_Zones_December_2011_Generalised_Clipped_Boundaries_in_England_and_Wales.shp"), 
  stringsAsFactors = F, quiet = T), crs=27700)[,c("wz11cd", "lad11cd", "lad11nm")]
WZ.db <- WZ.db[WZ.db$lad11cd %in% regions$LAD_CD,]
WZ.db$area_ha <- as.numeric(st_area(WZ.db))*0.0001
WZ.db <- merge(WZ.db, regions, all.x=T, sort=F, by.x="lad11cd", by.y="LAD_CD")

t <- fread(paste0(wd.data, "/WP102EW.csv"))[,c("WZ_CD", "Emp_2011")]
WZ.db <- merge(WZ.db, t, all.x=T, sort=F, by.x="wz11cd", by.y="WZ_CD"); rm(t)

names(WZ.db)[names(WZ.db)=="wz11cd"] <- "WZ_CD"
names(WZ.db)[names(WZ.db)=="lad11cd"] <- "LAD_CD"
names(WZ.db)[names(WZ.db)=="lad11nm"] <- "LAD_NM"
names(WZ.db)[names(WZ.db)=="Emp_2011"] <- "employment" 

t <- st_cast(WZ.db, "POLYGON")
t1 <- t[t$WZ_CD %in% t[which(duplicated(t$WZ_CD)),]$WZ_CD,]
t <- t[!(t$WZ_CD %in% t1$WZ_CD),]
t1$area_ha <- as.numeric(st_area(t1))*0.0001
t2 <- t1 %>% 
  group_by(WZ_CD) %>%
  filter(area_ha == max(area_ha)) %>%
  arrange(WZ_CD)
t2 <- st_sf(data.frame(t2))
WZ.db <- rbind(t, t2)
rm(t, t1, t2)
```

We are interested in the small area spatial distribution of employment across England. We'll need three data sources for this work, all openly available and accessible.


* Workplace Zone employment data for England (local area working day population) [ONS Nomis](https://www.nomisweb.co.uk/ "ONS Nomis Labour Market Statistics")
    + Table WP102EW (Census 2011)
    
* Workplace Zone spatial boundary file for England [ONS Open Geography Portal](https://geoportal.statistics.gov.uk/ "The Open Geography Portal")
    + *From this we can derive area, distance and density measures for each tract*

* Lookup tables between Workplace Zone, Local Authority, Regions, and Combined Authority


This analysis is tested across multiple distinct regions (urban areas) in England - taken as the Combined Authority geographies. Subcentre identification is applied region by region across the country with tailored parameters to best fit local magnitudes.


We can calculate and present the code to generate aggregate employment levels.

```{r WZ Employment Statistics by Region}
WZ.db$density <- ifelse(is.na(WZ.db$employment/WZ.db$area_ha), 0, WZ.db$employment/WZ.db$area_ha)

WZ.db$dRNK.nat <- cut(WZ.db$density, breaks = quantile(WZ.db$density, probs = 0:10/10, na.rm=T), labels = 1:10, right = FALSE, include.lowest = TRUE)
WZ.db <- do.call(rbind, lapply(split(WZ.db, WZ.db$Region), function(x) { x$dRNK.lcl <- cut(x$density, breaks = quantile(x$density, probs = 0:10/10, na.rm=T), labels = 1:10, right = FALSE, include.lowest = TRUE); return(x) })); rownames(WZ.db) <- NULL

WZ.stats <- as_tibble(WZ.db) %>%
  dplyr::group_by(Region) %>%
  dplyr::summarize(Total_Area=sum(area_ha), Avg_Area=mean(area_ha), Med_Area=median(area_ha), Min_Area=min(area_ha), Max_Area=max(area_ha), SD_Area=sd(area_ha),
    Total_Emp=sum(employment), Avg_Emp=mean(employment), Med_Emp=median(employment), Min_Emp=min(employment), Max_Emp=max(employment), SD_Emp=sd(employment),
    Avg_Dens=sum(density), Med_Dens=mean(density), Min_Dens=min(density), Max_Dens=max(density), SD_Dens=sd(density), WZ_count=length(WZ_CD)) %>%
  filter(!is.na(Region)) %>%
  arrange(desc(Total_Emp))

knitr::kable(WZ.stats[order(-WZ.stats$Total_Emp), c("Region", "Total_Emp", "Total_Area", "Avg_Dens", "WZ_count")])
```



### 2.2. Employment Density Plots

Plotting density decile ranks for each of the small area (WZ) areas across the city reveal hotspot patterns and spatial distributions of employment.

It is against these density plots that we want to evaluate the validity of the monocentric model - and to which we apply the subcentre identification algorithm.


```{r Figure 2a, fig.cap="Figure 2: Liverpool Employment Densities"}
density.plots <- lapply(split(WZ.db, WZ.db$Region), function(x){
  ggplot() +
    geom_sf(data = x, aes(fill = dRNK.lcl), lwd = 0) +
    scale_fill_manual(values=colorRampPalette(c("royalblue", "springgreen", "yellow", "red"))(10), na.translate=FALSE) +
    labs(title = x$Region[1], subtitle = "Local Employment Density Deciles") +
    scale_x_continuous(expand = c(0,0)) +
    coord_sf() +
    guides(shape = guide_legend(override.aes = list(size = 1)), fill = guide_legend(override.aes = list(size = 1)),
      color = guide_legend(override.aes = list(size = 1))) +
    theme_classic() +
    theme(axis.text.x=element_blank(), axis.ticks.x=element_blank(),
      axis.text.y=element_blank(), axis.ticks.y=element_blank(), legend.position="bottom", legend.title = element_blank())
})
names(density.plots) <- names(split(WZ.db, WZ.db$Region))

p <- density.plots[["Liverpool City Region"]]
p
```

```{r Figure 2b, fig.cap="Figure 2: Manchester Employment Densities"}
p <- density.plots[["Greater Manchester"]]
p
```

```{r Figure 2c, fig.cap="Figure 2: London Employment Densities"}
p <- density.plots[["London"]]
p
```




## Section 3: England Regional Employment Subcentre Identification

```{r Source Subcentre Identification Algorithm, include=FALSE}
source(paste0(wd, "/Code/Subcentre_Identification.R"))

# We source the Subcentre_Identification.R function located in the Code folder. This function reads in a cleaned shapefile to identify employment subcentres based on our choice of functional form and input parameters. 

WZ.db <- st_transform(WZ.db, crs=4326)
WZ.db2 <- lapply(split(WZ.db, WZ.db$Region), function(x) {x$dist_CBD <- as.numeric(st_distance(x, x[which.max(x$density),], which ="Great Circle"))*0.001; return(x)})
WZ.db <- do.call(rbind, WZ.db2)
rownames(WZ.db) <- NULL
rm(WZ.db2)

WZ.db$dist_CBD.sq <- WZ.db$dist_CBD*WZ.db$dist_CBD
WZ.db$dist_CBD.cb <- WZ.db$dist_CBD.sq*WZ.db$dist_CBD
```

### 3.1. Estimated Employment Density Gradients by Region (Urban Area)


Included in this repository is the subcentre identification algorithm tool itself located in [Subcentre_Identification.R](https://github.com/jacobmacdonald02/OpenResearchWeek_2021/blob/master/Code/Subcentre_Identification.R "Identifying Employment Subcentre Identification Algorithm")

This updated version of [Ban et al. 2017](https://www.mdpi.com/2073-445X/6/1/17/htm "Identifying Employment Subcenters: The Method of Exponentially Declining Cutoffs") can be applied to the different regions across England to systematically identify where we have **employment hotspots**

We can estimate and evaluate the fit of each of the assumed model functional forms. We want to make sure we select the most appropriate form against which to look for outlier employment hotspots.


```{r Generate Estimated Gradient Values, include=FALSE}
density.linear <- lapply(split(WZ.db, WZ.db$Region), function(x) lm(log(ifelse(x$density > 0, x$density, 0.5)) ~ x$dist_CBD))
density.linear <- lapply(as.list(1:length(density.linear)), function(g) {
  t <- as.data.frame(cbind(Region=names(density.linear)[g], Model="Linear", x=seq(0.01, 200, by = 0.01), y=exp(as.numeric(summary(density.linear[[g]])$coefficients[1,1]))*exp(as.numeric(summary(density.linear[[g]])$coefficients[2,1])*seq(0.01, 200, by = 0.01)), Rsq=summary(density.linear[[g]])$adj.r.squared))
  t$x <- as.numeric(as.character(t$x))
  t$y <- as.numeric(as.character(t$y))
  t$Rsq <- as.numeric(as.character(t$Rsq))
  t$Region <- as.factor(t$Region)
  t$Model <- as.factor(t$Model)
  return(t)
  })
density.linear <- do.call(rbind, density.linear)

density.squared <- lapply(split(WZ.db, WZ.db$Region), function(x) lm(log(ifelse(x$density > 0, x$density, 0.5)) ~ x$dist_CBD + x$dist_CBD.sq))
density.squared <- lapply(as.list(1:length(density.squared)), function(g) {
  t <- as.data.frame(cbind(Region=names(density.squared)[g], Model="Squared", x=seq(0.01, 200, by = 0.01), y=exp(as.numeric(summary(density.squared[[g]])$coefficients[1,1]))*exp(as.numeric(summary(density.squared[[g]])$coefficients[2,1])*seq(0.01, 200, by = 0.01) + as.numeric(summary(density.squared[[g]])$coefficients[3,1])*(seq(0.01, 200, by = 0.01)^2)), Rsq=summary(density.squared[[g]])$adj.r.squared))
  t$x <- as.numeric(as.character(t$x))
  t$y <- as.numeric(as.character(t$y))
  t$Rsq <- as.numeric(as.character(t$Rsq))
  t$Region <- as.factor(t$Region)
  t$Model <- as.factor(t$Model)
  return(t)
  })
density.squared <- do.call(rbind, density.squared)

density.cubed <- lapply(split(WZ.db, WZ.db$Region), function(x) lm(log(ifelse(x$density > 0, x$density, 0.5)) ~ x$dist_CBD + x$dist_CBD.sq + x$dist_CBD.cb))
density.cubed <- lapply(as.list(1:length(density.cubed)), function(g) {
  t <- as.data.frame(cbind(Region=names(density.cubed)[g], Model="Cubed", x=seq(0.01, 200, by = 0.01), y=exp(as.numeric(summary(density.cubed[[g]])$coefficients[1,1]))*exp(as.numeric(summary(density.cubed[[g]])$coefficients[2,1])*seq(0.01, 200, by = 0.01) + as.numeric(summary(density.cubed[[g]])$coefficients[3,1])*(seq(0.01, 200, by = 0.01)^2) + as.numeric(summary(density.cubed[[g]])$coefficients[4,1])*(seq(0.01, 200, by = 0.01)^3)), Rsq=summary(density.cubed[[g]])$adj.r.squared) )
  t$x <- as.numeric(as.character(t$x))
  t$y <- as.numeric(as.character(t$y))
  t$Rsq <- as.numeric(as.character(t$Rsq))
  t$Region <- as.factor(t$Region)
  t$Model <- as.factor(t$Model)
  return(t)
  })
density.cubed <- do.call(rbind, density.cubed)

density.log <- lapply(split(WZ.db, WZ.db$Region), function(x) lm(log(ifelse(x$density > 0, x$density, 0.5)) ~ log(ifelse(x$dist_CBD > 0, x$dist_CBD, 0.5))))
density.log <- lapply(as.list(1:length(density.log)), function(g) {
  t <- as.data.frame(cbind(Region=names(density.log)[g], Model="Log", x=seq(0.01, 200, by = 0.01), y=exp(as.numeric(summary(density.log[[g]])$coefficients[1,1]) + as.numeric(summary(density.log[[g]])$coefficients[2,1])*log(seq(0.01, 200, by = 0.01))), Rsq=summary(density.log[[g]])$adj.r.squared))
  t$x <- as.numeric(as.character(t$x))
  t$y <- as.numeric(as.character(t$y))
  t$Rsq <- as.numeric(as.character(t$Rsq))
  t$Region <- as.factor(t$Region)
  t$Model <- as.factor(t$Model)
  return(t)
  })
density.log <- do.call(rbind, density.log)

density.plots <- rbind(density.linear, density.squared)
density.plots <- rbind(density.plots, density.cubed)
density.plots <- rbind(density.plots, density.log)
rm(density.linear, density.squared, density.cubed, density.log)
```

```{r Export Density Gradient Plots, include=FALSE}
plot.params <- data.frame(Region=unique(regions$Region))
plot.params$y.lim <- 175
plot.params$y.lim[plot.params$Region %in% c("Cambridgeshire and Peterborough", "North East", "Sheffield City Region", "Tees Valley")] <- 50
plot.params$y.lim[plot.params$Region %in% c("London")] <- 1000
plot.params$x.lim <- 40
plot.params$x.lim[plot.params$Region %in% c("London", "Cambridgeshire and Peterborough", "North of Tyne", "Tees Valley", "West of England")] <- 20

estimated.density <- lapply(as.list(unique(as.character(regions$Region))), function(p) {
  gradient.plots <- ggplot(data=density.plots[density.plots$Region==p,], aes(x=x)) +
    geom_line(aes(y = y, group = Model, color = Model)) +
    scale_color_manual(values = c("#d54062", "#ffa36c", "#ebdc87", "#799351")) +
    scale_y_continuous(limits = c(0, plot.params[plot.params$Region==p,]$y.lim), expand = c(0,0)) +
    scale_x_continuous(limits = c(0, plot.params[plot.params$Region==p,]$x.lim), expand = c(0,0)) +
    labs(y = "Density per ha.", x = "Distance to City Centre (km)", fill = "Functional",
      title = "Estimated Employment Density Gradients", subtitle = paste0(p, ": (N = ", as.character(WZ.stats[WZ.stats$Region==p,]$WZ_count), ")"),
      caption = paste0("R-Squared: Linear=", unique(signif(density.plots[density.plots$Region==p & density.plots$Model=="Linear",]$Rsq, 4)),
  " ; Squared=", unique(signif(density.plots[density.plots$Region==p & density.plots$Model=="Squared",]$Rsq, 4)),
  " ; Cubed=", unique(signif(density.plots[density.plots$Region==p & density.plots$Model=="Cubed",]$Rsq, 4)),
  " ; Log=", unique(signif(density.plots[density.plots$Region==p & density.plots$Model=="Log",]$Rsq, 4)))) +
    theme_classic() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1), legend.position="bottom", plot.caption = element_text(face = "italic"), legend.title = element_blank())
})
rm(plot.params)
names(estimated.density) <- unique(as.character(regions$Region))
```



```{r Figure 4a, fig.cap="Figure 4: Liverpool Density Gradients"}
p <- estimated.density[["Liverpool City Region"]]
p
```

```{r Figure 4b, fig.cap="Figure 4: Manchester Density Gradients"}
p <- estimated.density[["Greater Manchester"]]
p
```

```{r Figure 4c, fig.cap="Figure 4: London Density Gradients"}
p <- estimated.density[["London"]]
p
```


We can also plot the estimated model results from fitting the gradient along the different types of functional form, which helps us evaluate which model we should be choosing. 

```{r Density Gradient Model Fits, include=FALSE}
t <- unique(density.plots[,c("Region", "Model", "Rsq")])
t <- split(t, t$Region)
t <- lapply(t, function(x) {x$max <- ifelse(x$Rsq==max(x$Rsq), 1, 0); return(x)})
t <- as.data.frame(do.call(rbind, t))
rownames(t) <- NULL
```


```{r Density Gradient Model Fits Plot}
p <- ggplot() +
  geom_point(data=t[t$max==1,], aes(x = Region, y = Rsq, fill=Model, group = Model, color = Model)) +
  scale_color_manual(values = c("#d54062", "#ffa36c", "#ebdc87", "#799351")) +
  geom_point(data=t[t$max==0,], aes(x = Region, y = Rsq, fill=Model, group = Model, color = Model), alpha=0.3) +
  scale_color_manual(values = c("#d54062", "#ffa36c", "#ebdc87", "#799351")) +
  labs(y = "R Squared", x = "Region", title = "Estimated Density Gradient Fits") +
  theme_classic() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1), legend.position="bottom", legend.title = element_blank())
p
```



### 3.2. Subcentre Identification

Included in this repository is the subcentre identification algorithm tool itself located in [Subcentre_Identification.R](https://github.com/jacobmacdonald02/OpenResearchWeek_2021/blob/master/Code/Subcentre_Identification.R "Identifying Employment Subcentre Identification Algorithm")


```{r Subcentre Identification, include=FALSE}
model.parameters <- expand.grid(unique(WZ.db$Region)[c(3,4)], c("Cubed"), NA, NA)
names(model.parameters) <- c("Region", "Model_Gradient", "Density_Cutoff", "Employment_Cutoff")
model.parameters$Density_Cutoff <- c(5, 3)
model.parameters$Employment_Cutoff <- c(0.1, 0.1)

subcentre <- lapply(split(model.parameters, 1:nrow(model.parameters)), function(x) {
  subcentre.identification(WZ.db[WZ.db$Region==as.character(x[,c("Region")]),], c("WZ_CD", "employment", "area_ha"), GRADIENT=as.character(x[,c("Model_Gradient")]), CBD.cutD=as.numeric(x[,"Density_Cutoff"]), CBD.cutE=as.numeric(x[,"Employment_Cutoff"]) )
})

liverpool.subcentres <- subcentre[[1]][,c("tractID", "candidateID", "subcentreID", "SC_employment", "SC_area", "SC_distance", "SC_density", "tractN")]
names(liverpool.subcentres)[names(liverpool.subcentres)=="tractID"] <- c("WZ_CD")
liverpool.subcentres$candidateID[!is.na(liverpool.subcentres$subcentreID)] <- NA
liverpool.subcentres$subcentreC <- 0
liverpool.subcentres$subcentreC[!is.na(liverpool.subcentres$candidateID)] <- 1
liverpool.subcentres$subcentreC[!is.na(liverpool.subcentres$subcentreID)] <- 2

london.subcentres <- subcentre[[2]][,c("tractID", "candidateID", "subcentreID", "SC_employment", "SC_area", "SC_distance", "SC_density", "tractN")]
names(london.subcentres)[names(london.subcentres)=="tractID"] <- c("WZ_CD")
london.subcentres$candidateID[!is.na(london.subcentres$subcentreID)] <- NA
london.subcentres$subcentreC <- 0
london.subcentres$subcentreC[!is.na(london.subcentres$candidateID)] <- 1
london.subcentres$subcentreC[!is.na(london.subcentres$subcentreID)] <- 2

subcentre <- list(liverpool.subcentres, london.subcentres)
```

```{r Subcentre Plots, include=FALSE}
SC.plot <- lapply(as.list(1:length(subcentre)), function(y) {
  ggplot() +
    geom_sf(data = subcentre[[y]], color = "gray80", fill = "gray83") +
    geom_sf(data = subcentre[[y]][subcentre[[y]]$subcentreC==1,], color = "gold", fill= "gold", show.legend = FALSE, lwd = 0.01) +
    geom_sf(data = subcentre[[y]][subcentre[[y]]$subcentreC==2 & !is.na(subcentre[[y]]$subcentreC),], aes(fill=subcentreID), color = "red4", show.legend = FALSE, lwd = 0.01) +
    scale_fill_manual(values=sample(colorRampPalette(colors = c("#DC1C13", "#F6BDC0"))(20), length(unique(subcentre[[y]][subcentre[[y]]$subcentreC==2 & !is.na(subcentre[[y]]$subcentreC),]$subcentreID)), replace=TRUE)) +
    labs(title=as.character(model.parameters[y,1]), subtitle=paste0(as.character(model.parameters[y,2]), " Functional Form")) +
    theme(axis.text.x = element_blank(), axis.text.y = element_blank(), axis.ticks = element_blank(), plot.caption = element_text(face = "italic"))
})
```


While these applications are relatively simple 'out-of-the-box' application of the algorithm, the flexibility, speed, and openess of the data and research means that it can serve as a benchmark tool applied quickly, consistently and easily modified to different local contexts. 

```{r Figure 5a, fig.cap="Figure 4: Liverpool Density Gradients"}
p <- SC.plot[[1]]
p
```


```{r Figure 5b, fig.cap="Figure 4: London Density Gradients"}
p <- SC.plot[[2]]
p
```




## Selected References

[Ban, Jifei, Richard Arnott, and Jacob L. Macdonald. 2017. "Identifying Employment Subcenters: The Method of Exponentially Declining Cutoffs." Land 6 (17).](https://www.mdpi.com/2073-445X/6/1/17/htm) 

https://happygitwithr.com/

[Office for National Statistics. 2014. "Workplace Zones: A new geography for workplace statistics."](https://data.gov.uk/dataset/6620567e-f237-4c6b-b561-64a2bc218783/workplace-zones-a-new-geography-for-workplace-statistics)

[Office for National Statistics Nomis](https://www.nomisweb.co.uk/ "ONS Nomis Labour Market Statistics")

[Office for National Statistics Open Geography Portal](https://geoportal.statistics.gov.uk/ "The Open Geography Portal")



## Links From The Slides


